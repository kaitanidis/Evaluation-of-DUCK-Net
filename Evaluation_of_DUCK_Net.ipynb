{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4022c4cf",
      "metadata": {},
      "source": [
        "Το link αμέσως μετά για το colab θέλει διόρθωση ώστε να μην λέει backgroundgenerator..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaitanidis/BackGroundGenerator/blob/main/Evaluation_of_DUCK_Net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_-G8PhZVAaDQ",
      "metadata": {
        "id": "_-G8PhZVAaDQ"
      },
      "source": [
        "Dumitru, RG., Peteleaza, D. & Craciun, C. Using DUCK-Net for polyp image segmentation. Sci Rep 13, 9803 (2023). https://doi.org/10.1038/s41598-023-36940-5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VMDo9_ikbkkT",
      "metadata": {
        "id": "VMDo9_ikbkkT"
      },
      "source": [
        "https://github.com/RazvanDu/DUCK-Net/tree/main"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hBq6SDvBdfST",
      "metadata": {
        "id": "hBq6SDvBdfST"
      },
      "source": [
        "Kvasir SEG\n",
        "Segmented Polyp Dataset for Computer Aided Gastrointestinal Disease Detection.\n",
        "https://datasets.simula.no/kvasir-seg/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d69d8fd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Αν δεν παίζει με GPUs το Colab, τρέξτε αυτό το κελί\n",
        "# Δες σχόλια εδώ: https://stackoverflow.com/questions/78203005/google-colab-cant-use-the-gpu\n",
        "\n",
        "# !apt update && apt install cuda-11-8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0tKQAzYv9c9u",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tKQAzYv9c9u",
        "outputId": "4c50f933-4a04-4540-9baf-f85632e14f2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: tensorflow==2.10.0 in /usr/local/lib/python3.10/dist-packages (2.10.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (24.3.25)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.68.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (3.12.1)\n",
            "Requirement already satisfied: keras<2.11,>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (2.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.1.2)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (18.1.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (24.2)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (3.19.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.17.0)\n",
            "Requirement already satisfied: tensorboard<2.11,>=2.10 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (2.10.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (0.37.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (2.10.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.10.0) (1.17.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.10.0) (0.45.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10.0) (3.2.2)\n",
            "Requirement already satisfied: scikit-learn==1.1.1 in /usr/local/lib/python3.10/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.1) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.1) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.1) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.1.1) (3.5.0)\n",
            "Requirement already satisfied: albumentations==1.2.1 in /usr/local/lib/python3.10/dist-packages (1.2.1)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from albumentations==1.2.1) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from albumentations==1.2.1) (1.13.1)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.10/dist-packages (from albumentations==1.2.1) (0.19.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations==1.2.1) (6.0.2)\n",
            "Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from albumentations==1.2.1) (0.0.4)\n",
            "Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from albumentations==1.2.1) (4.10.0.84)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from qudida>=0.0.4->albumentations==1.2.1) (1.1.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from qudida>=0.0.4->albumentations==1.2.1) (4.12.2)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations==1.2.1) (3.4.2)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations==1.2.1) (9.2.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations==1.2.1) (2.36.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations==1.2.1) (2024.12.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations==1.2.1) (1.8.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations==1.2.1) (24.2)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.2.1) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.2.1) (3.5.0)\n",
            "Requirement already satisfied: opencv-python==4.6.0.66 in /usr/local/lib/python3.10/dist-packages (4.6.0.66)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python==4.6.0.66) (1.23.5)\n",
            "Requirement already satisfied: matplotlib==3.5.2 in /usr/local/lib/python3.10/dist-packages (3.5.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.2) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.2) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.2) (1.4.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.2) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.2) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.2) (9.2.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.2) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.2) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib==3.5.2) (1.17.0)\n",
            "Requirement already satisfied: tqdm==4.64.0 in /usr/local/lib/python3.10/dist-packages (4.64.0)\n",
            "Requirement already satisfied: Pillow==9.2.0 in /usr/local/lib/python3.10/dist-packages (9.2.0)\n",
            "Requirement already satisfied: scikit-image==0.19.3 in /usr/local/lib/python3.10/dist-packages (0.19.3)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image==0.19.3) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image==0.19.3) (1.13.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image==0.19.3) (3.4.2)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image==0.19.3) (9.2.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image==0.19.3) (2.36.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image==0.19.3) (2024.12.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image==0.19.3) (1.8.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image==0.19.3) (24.2)\n",
            "Requirement already satisfied: keras==2.10.0 in /usr/local/lib/python3.10/dist-packages (2.10.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy==1.23.5      # ΠΡΟΣΟΧΗ στα requirments λένε 1.23.1 αλλά δεν παίζει έτσι.\n",
        "!pip install tensorflow==2.10.0\n",
        "!pip install scikit-learn==1.1.1\n",
        "!pip install albumentations==1.2.1\n",
        "!pip install opencv-python==4.6.0.66\n",
        "!pip install matplotlib==3.5.2\n",
        "!pip install tqdm==4.64.0\n",
        "!pip install Pillow==9.2.0\n",
        "!pip install scikit-image==0.19.3\n",
        "!pip install keras==2.10.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "YrGNiA2F_SwX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrGNiA2F_SwX",
        "outputId": "36de917e-11cc-4f60-907a-c2f229ea0c75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'DUCK-Net' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "#!git clone https://github.com/RazvanDu/DUCK-Net.git\n",
        "!git clone https://github.com/kaitanidis/Evaluation-of-DUCK-Net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "857a5f70",
      "metadata": {
        "id": "857a5f70"
      },
      "outputs": [],
      "source": [
        "# Importing the necessary libraries\n",
        "\n",
        "import tensorflow as tf\n",
        "import albumentations as albu\n",
        "import numpy as np\n",
        "import gc\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.callbacks import CSVLogger\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import jaccard_score, precision_score, recall_score, accuracy_score, f1_score\n",
        "from ModelArchitecture.DiceLoss import dice_metric_loss\n",
        "from ModelArchitecture import DUCK_Net\n",
        "from ImageLoader import ImageLoader2D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7852dbbf-f3a4-4db6-8c5e-98889bf3abd4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7852dbbf-f3a4-4db6-8c5e-98889bf3abd4",
        "outputId": "1a63539e-ce82-4d52-940a-63bd01e34c0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  0\n"
          ]
        }
      ],
      "source": [
        "# Checking the number of GPUs available\n",
        "\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9edbe667",
      "metadata": {
        "id": "9edbe667"
      },
      "outputs": [],
      "source": [
        "# Setting the model parameters\n",
        "\n",
        "img_size = 352\n",
        "dataset_type = 'kvasir' # Options: kvasir/cvc-clinicdb/cvc-colondb/etis-laribpolypdb\n",
        "learning_rate = 1e-3\n",
        "seed_value = 58800\n",
        "filters = 17 # Number of filters, the paper presents the results with 17 and 34\n",
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
        "\n",
        "ct = datetime.now()\n",
        "\n",
        "model_type = \"DuckNet\"\n",
        "\n",
        "progress_path = 'ProgressFull/' + dataset_type + '_progress_csv_' + model_type + '_filters_' + str(filters) +  '_' + str(ct) + '.csv'\n",
        "progressfull_path = 'ProgressFull/' + dataset_type + '_progress_' + model_type + '_filters_' + str(filters) + '_' + str(ct) + '.txt'\n",
        "plot_path = 'ProgressFull/' + dataset_type + '_progress_plot_' + model_type + '_filters_' + str(filters) + '_' + str(ct) + '.png'\n",
        "model_path = 'ModelSaveTensorFlow/' + dataset_type + '/' + model_type + '_filters_' + str(filters) + '_' + str(ct)\n",
        "\n",
        "EPOCHS = 10\n",
        "min_loss_for_saving = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "264050d8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "264050d8",
        "outputId": "186d9bc2-2f75-4f8f-864e-18a15eaac630"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resizing training images and masks: 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "5it [00:02,  1.87it/s]\n"
          ]
        }
      ],
      "source": [
        "# Loading the data\n",
        "\n",
        "X, Y = ImageLoader2D.load_data(img_size, img_size, -1, 'kvasir')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75b6c029",
      "metadata": {
        "id": "75b6c029"
      },
      "outputs": [],
      "source": [
        "# Splitting the data, seed for reproducibility\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, shuffle= True, random_state = seed_value)\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.111, shuffle= True, random_state = seed_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92a7b7d0",
      "metadata": {
        "id": "92a7b7d0"
      },
      "outputs": [],
      "source": [
        "# Defining the augmentations\n",
        "\n",
        "aug_train = albu.Compose([\n",
        "    albu.HorizontalFlip(),\n",
        "    albu.VerticalFlip(),\n",
        "    albu.ColorJitter(brightness=(0.6,1.6), contrast=0.2, saturation=0.1, hue=0.01, always_apply=True),\n",
        "    albu.Affine(scale=(0.5,1.5), translate_percent=(-0.125,0.125), rotate=(-180,180), shear=(-22.5,22), always_apply=True),\n",
        "])\n",
        "\n",
        "def augment_images():\n",
        "    x_train_out = []\n",
        "    y_train_out = []\n",
        "\n",
        "    for i in range (len(x_train)):\n",
        "        ug = aug_train(image=x_train[i], mask=y_train[i])\n",
        "        x_train_out.append(ug['image'])\n",
        "        y_train_out.append(ug['mask'])\n",
        "\n",
        "    return np.array(x_train_out), np.array(y_train_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1609dd32",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1609dd32",
        "outputId": "688f5ddc-2447-4e65-cb1b-683dd7330191"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting DUCK-Net\n"
          ]
        }
      ],
      "source": [
        "# Creating the model\n",
        "\n",
        "model = DUCK_Net.create_model(img_height=img_size, img_width=img_size, input_chanels=3, out_classes=1, starting_filters=filters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e513d42",
      "metadata": {
        "id": "2e513d42"
      },
      "outputs": [],
      "source": [
        "# Compiling the model\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=dice_metric_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ef712b9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ef712b9",
        "outputId": "0553cff2-3a2b-4602-bd66-f900dbd0b599"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training, epoch 0\n",
            "Learning Rate: 0.0001\n",
            "1/1 [==============================] - 40s 40s/step - loss: 0.6025 - val_loss: 1.0000\n",
            "Loss Validation: 1.0\n",
            "Loss Test: 1.0\n",
            "Training, epoch 1\n",
            "Learning Rate: 0.0001\n",
            "1/1 [==============================] - 39s 39s/step - loss: 0.6965 - val_loss: 1.0000\n",
            "Loss Validation: 1.0\n",
            "Loss Test: 1.0\n",
            "Training, epoch 2\n",
            "Learning Rate: 0.0001\n",
            "1/1 [==============================] - 39s 39s/step - loss: 0.6976 - val_loss: 1.0000\n",
            "Loss Validation: 1.0\n",
            "Loss Test: 1.0\n"
          ]
        }
      ],
      "source": [
        "# Training the model\n",
        "import os\n",
        "\n",
        "step = 0\n",
        "\n",
        "for epoch in range(0, EPOCHS):\n",
        "\n",
        "    print(f'Training, epoch {epoch}')\n",
        "    print('Learning Rate: ' + str(learning_rate))\n",
        "\n",
        "    step += 1\n",
        "\n",
        "    image_augmented, mask_augmented = augment_images()\n",
        "\n",
        "    # Create the 'ProgressFull' directory if it doesn't exist\n",
        "    os.makedirs(os.path.dirname(progress_path), exist_ok=True)\n",
        "\n",
        "    csv_logger = CSVLogger(progress_path, append=True, separator=';')\n",
        "\n",
        "    model.fit(x=image_augmented, y=mask_augmented, epochs=1, batch_size=4, validation_data=(x_valid, y_valid), verbose=1, callbacks=[csv_logger])\n",
        "\n",
        "    prediction_valid = model.predict(x_valid, verbose=0)\n",
        "    loss_valid = dice_metric_loss(y_valid, prediction_valid)\n",
        "\n",
        "    loss_valid = loss_valid.numpy()\n",
        "    print(\"Loss Validation: \" + str(loss_valid))\n",
        "\n",
        "    prediction_test = model.predict(x_test, verbose=0)\n",
        "    loss_test = dice_metric_loss(y_test, prediction_test)\n",
        "    loss_test = loss_test.numpy()\n",
        "    print(\"Loss Test: \" + str(loss_test))\n",
        "\n",
        "    with open(progressfull_path, 'a') as f:\n",
        "        f.write('epoch: ' + str(epoch) + '\\nval_loss: ' + str(loss_valid) + '\\ntest_loss: ' + str(loss_test) + '\\n\\n\\n')\n",
        "\n",
        "    if min_loss_for_saving > loss_valid:\n",
        "        min_loss_for_saving = loss_valid\n",
        "        print(\"Saved model with val_loss: \", loss_valid)\n",
        "\n",
        "        # Ensure the directory exists before saving\n",
        "        os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
        "\n",
        "        model.save(model_path)\n",
        "\n",
        "    del image_augmented\n",
        "    del mask_augmented\n",
        "\n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cd52447",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "2cd52447",
        "outputId": "686f55ae-f983-45ea-9a1c-0b7b28762196"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading the model\n"
          ]
        },
        {
          "ename": "OSError",
          "evalue": "No file or directory found at ModelSaveTensorFlow/kvasir/DuckNet_filters_17_2025-01-12 12:48:14.843479",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-1b1ca0b8fff2>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading the model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'dice_metric_loss'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdice_metric_loss\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprediction_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m                         raise IOError(\n\u001b[0m\u001b[1;32m    227\u001b[0m                             \u001b[0;34mf\"No file or directory found at {filepath_str}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m                         )\n",
            "\u001b[0;31mOSError\u001b[0m: No file or directory found at ModelSaveTensorFlow/kvasir/DuckNet_filters_17_2025-01-12 12:48:14.843479"
          ]
        }
      ],
      "source": [
        "# Computing the metrics and saving the results\n",
        "\n",
        "print(\"Loading the model\")\n",
        "\n",
        "model = tf.keras.models.load_model(model_path, custom_objects={'dice_metric_loss':dice_metric_loss})\n",
        "\n",
        "prediction_train = model.predict(x_train, batch_size=4)\n",
        "prediction_valid = model.predict(x_valid, batch_size=4)\n",
        "prediction_test = model.predict(x_test, batch_size=4)\n",
        "\n",
        "print(\"Predictions done\")\n",
        "\n",
        "dice_train = f1_score(np.ndarray.flatten(np.array(y_train, dtype=bool)),\n",
        "                           np.ndarray.flatten(prediction_train > 0.5))\n",
        "dice_test = f1_score(np.ndarray.flatten(np.array(y_test, dtype=bool)),\n",
        "                          np.ndarray.flatten(prediction_test > 0.5))\n",
        "dice_valid = f1_score(np.ndarray.flatten(np.array(y_valid, dtype=bool)),\n",
        "                           np.ndarray.flatten(prediction_valid > 0.5))\n",
        "\n",
        "print(\"Dice finished\")\n",
        "\n",
        "\n",
        "miou_train = jaccard_score(np.ndarray.flatten(np.array(y_train, dtype=bool)),\n",
        "                           np.ndarray.flatten(prediction_train > 0.5))\n",
        "miou_test = jaccard_score(np.ndarray.flatten(np.array(y_test, dtype=bool)),\n",
        "                          np.ndarray.flatten(prediction_test > 0.5))\n",
        "miou_valid = jaccard_score(np.ndarray.flatten(np.array(y_valid, dtype=bool)),\n",
        "                           np.ndarray.flatten(prediction_valid > 0.5))\n",
        "\n",
        "print(\"Miou finished\")\n",
        "\n",
        "\n",
        "precision_train = precision_score(np.ndarray.flatten(np.array(y_train, dtype=bool)),\n",
        "                                  np.ndarray.flatten(prediction_train > 0.5))\n",
        "precision_test = precision_score(np.ndarray.flatten(np.array(y_test, dtype=bool)),\n",
        "                                 np.ndarray.flatten(prediction_test > 0.5))\n",
        "precision_valid = precision_score(np.ndarray.flatten(np.array(y_valid, dtype=bool)),\n",
        "                                  np.ndarray.flatten(prediction_valid > 0.5))\n",
        "\n",
        "print(\"Precision finished\")\n",
        "\n",
        "\n",
        "recall_train = recall_score(np.ndarray.flatten(np.array(y_train, dtype=bool)),\n",
        "                            np.ndarray.flatten(prediction_train > 0.5))\n",
        "recall_test = recall_score(np.ndarray.flatten(np.array(y_test, dtype=bool)),\n",
        "                           np.ndarray.flatten(prediction_test > 0.5))\n",
        "recall_valid = recall_score(np.ndarray.flatten(np.array(y_valid, dtype=bool)),\n",
        "                            np.ndarray.flatten(prediction_valid > 0.5))\n",
        "\n",
        "print(\"Recall finished\")\n",
        "\n",
        "\n",
        "accuracy_train = accuracy_score(np.ndarray.flatten(np.array(y_train, dtype=bool)),\n",
        "                                np.ndarray.flatten(prediction_train > 0.5))\n",
        "accuracy_test = accuracy_score(np.ndarray.flatten(np.array(y_test, dtype=bool)),\n",
        "                               np.ndarray.flatten(prediction_test > 0.5))\n",
        "accuracy_valid = accuracy_score(np.ndarray.flatten(np.array(y_valid, dtype=bool)),\n",
        "                                np.ndarray.flatten(prediction_valid > 0.5))\n",
        "\n",
        "\n",
        "print(\"Accuracy finished\")\n",
        "\n",
        "\n",
        "final_file = 'results_' + model_type + '_' + str(filters) + '_' + dataset_type + '.txt'\n",
        "print(final_file)\n",
        "\n",
        "with open(final_file, 'a') as f:\n",
        "    f.write(dataset_type + '\\n\\n')\n",
        "    f.write('dice_train: ' + str(dice_train) + ' dice_valid: ' + str(dice_valid) + ' dice_test: ' + str(dice_test) + '\\n\\n')\n",
        "    f.write('miou_train: ' + str(miou_train) + ' miou_valid: ' + str(miou_valid) + ' miou_test: ' + str(miou_test) + '\\n\\n')\n",
        "    f.write('precision_train: ' + str(precision_train) + ' precision_valid: ' + str(precision_valid) + ' precision_test: ' + str(precision_test) + '\\n\\n')\n",
        "    f.write('recall_train: ' + str(recall_train) + ' recall_valid: ' + str(recall_valid) + ' recall_test: ' + str(recall_test) + '\\n\\n')\n",
        "    f.write('accuracy_train: ' + str(accuracy_train) + ' accuracy_valid: ' + str(accuracy_valid) + ' accuracy_test: ' + str(accuracy_test) + '\\n\\n\\n\\n')\n",
        "\n",
        "print('File done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rsN7DnG-ph6F",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsN7DnG-ph6F",
        "outputId": "bcc19e50-013c-4f97-d504-a6dcea4ce880"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training, epoch 0\n",
            "Learning Rate: 0.0001\n",
            "1/1 [==============================] - 41s 41s/step - loss: 0.6917 - val_loss: 1.0000\n",
            "Loss Validation: 1.0\n",
            "Loss Test: 1.0\n",
            "Saved model with val_loss:  1.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 273). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training, epoch 1\n",
            "Learning Rate: 0.0001\n",
            "1/1 [==============================] - 39s 39s/step - loss: 0.7112 - val_loss: 1.0000\n",
            "Loss Validation: 1.0\n",
            "Loss Test: 1.0\n",
            "Saved model with val_loss:  1.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 273). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training, epoch 2\n",
            "Learning Rate: 0.0001\n",
            "1/1 [==============================] - 39s 39s/step - loss: 0.7031 - val_loss: 1.0000\n",
            "Loss Validation: 1.0\n",
            "Loss Test: 1.0\n",
            "Saved model with val_loss:  1.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 273). These functions will not be directly callable after loading.\n"
          ]
        }
      ],
      "source": [
        "# Training the model V2 - GEMINI\n",
        "import os\n",
        "\n",
        "step = 0\n",
        "\n",
        "for epoch in range(0, EPOCHS):\n",
        "\n",
        "    print(f'Training, epoch {epoch}')\n",
        "    print('Learning Rate: ' + str(learning_rate))\n",
        "\n",
        "    step += 1\n",
        "\n",
        "    image_augmented, mask_augmented = augment_images()\n",
        "\n",
        "    # Create the 'ProgressFull' directory if it doesn't exist\n",
        "    os.makedirs(os.path.dirname(progress_path), exist_ok=True)\n",
        "\n",
        "    csv_logger = CSVLogger(progress_path, append=True, separator=';')\n",
        "\n",
        "    model.fit(x=image_augmented, y=mask_augmented, epochs=1, batch_size=4, validation_data=(x_valid, y_valid), verbose=1, callbacks=[csv_logger])\n",
        "\n",
        "    prediction_valid = model.predict(x_valid, verbose=0)\n",
        "    loss_valid = dice_metric_loss(y_valid, prediction_valid)\n",
        "\n",
        "    loss_valid = loss_valid.numpy()\n",
        "    print(\"Loss Validation: \" + str(loss_valid))\n",
        "\n",
        "    prediction_test = model.predict(x_test, verbose=0)\n",
        "    loss_test = dice_metric_loss(y_test, prediction_test)\n",
        "    loss_test = loss_test.numpy()\n",
        "    print(\"Loss Test: \" + str(loss_test))\n",
        "\n",
        "    with open(progressfull_path, 'a') as f:\n",
        "        f.write('epoch: ' + str(epoch) + '\\nval_loss: ' + str(loss_valid) + '\\ntest_loss: ' + str(loss_test) + '\\n\\n\\n')\n",
        "\n",
        "    #Always save the model at the end of every epoch for debugging\n",
        "    print(\"Saved model with val_loss: \", loss_valid)\n",
        "\n",
        "    # Ensure the directory exists before saving\n",
        "    os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
        "\n",
        "    model.save(model_path)\n",
        "\n",
        "    del image_augmented\n",
        "    del mask_augmented\n",
        "\n",
        "    gc.collect()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
